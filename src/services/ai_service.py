from typing import List, Dict, Optional, Union
import json
import time
import random
import threading
from pathlib import Path
from dataclasses import dataclass
from datetime import datetime
import asyncio
import concurrent.futures
from google import genai
from google.genai import types


@dataclass
class ConversationMessage:
    """ä¼šè©±ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸"""
    role: str  # "user" or "model"
    content: str
    timestamp: datetime


@dataclass
class TaskInfo:
    """ã‚¿ã‚¹ã‚¯æƒ…å ±"""
    title: str
    task_type: Optional[str] = None
    urgency: Optional[str] = None
    due_date: Optional[str] = None
    current_description: Optional[str] = None


@dataclass
class AIAnalysisResult:
    """AIåˆ†æçµæœ"""
    status: str  # "insufficient_info" or "ready_to_format"
    message: str
    suggestions: Optional[List[str]] = None
    formatted_content: Optional[str] = None


class ConversationHistory:
    """ä¼šè©±å±¥æ­´ç®¡ç†"""

    def __init__(self, storage_path: Optional[Union[str, Path]] = None):
        self.lock = threading.Lock()
        self.storage_path = Path(storage_path) if storage_path else Path(".ai_conversations.json")
        self.conversations: Dict[str, List[ConversationMessage]] = {}
        self._load_from_disk()

    def _load_from_disk(self):
        try:
            if self.storage_path.exists():
                data = json.loads(self.storage_path.read_text(encoding="utf-8"))
                for sid, msgs in data.items():
                    self.conversations[sid] = [
                        ConversationMessage(
                            role=m.get("role", "user"),
                            content=m.get("content", ""),
                            timestamp=datetime.fromisoformat(m.get("timestamp"))
                            if m.get("timestamp")
                            else datetime.now(),
                        )
                        for m in msgs
                    ]
        except Exception:
            # èª­ã¿è¾¼ã¿å¤±æ•—æ™‚ã¯ç©ºã¨ã—ã¦æ‰±ã†ï¼ˆå£Šã‚ŒãŸãƒ•ã‚¡ã‚¤ãƒ«ã§ã‚‚ç¨¼åƒã‚’æ­¢ã‚ãªã„ï¼‰
            self.conversations = {}

    def _flush_to_disk(self):
        try:
            payload = {
                sid: [
                    {
                        "role": m.role,
                        "content": m.content,
                        "timestamp": m.timestamp.isoformat(),
                    }
                    for m in msgs
                ]
                for sid, msgs in self.conversations.items()
            }
            tmp_path = self.storage_path.with_suffix(self.storage_path.suffix + ".tmp")
            tmp_path.write_text(json.dumps(payload, ensure_ascii=False), encoding="utf-8")
            tmp_path.replace(self.storage_path)
        except Exception:
            # æ›¸ãè¾¼ã¿å¤±æ•—ã¯è‡´å‘½çš„ã§ã¯ãªã„ãŸã‚æ¡ã‚Šã¤ã¶ã™ï¼ˆãƒ­ã‚°ã¯æ¨™æº–å‡ºåŠ›å´ã«ä»»ã›ã‚‹ï¼‰
            pass


class InMemoryConversationHistory:
    """ãƒ¡ãƒ¢ãƒªå†…ã®ã¿ã§ç®¡ç†ã™ã‚‹ä¼šè©±å±¥æ­´ï¼ˆä¸€æ™‚çš„ãªã‚»ãƒƒã‚·ãƒ§ãƒ³ç”¨ï¼‰"""

    def __init__(self):
        self.lock = threading.Lock()
        self.conversations: Dict[str, List[ConversationMessage]] = {}

    def add_message(self, session_id: str, role: str, content: str):
        """ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’è¿½åŠ """
        with self.lock:
            if session_id not in self.conversations:
                self.conversations[session_id] = []
            message = ConversationMessage(role=role, content=content, timestamp=datetime.now())
            self.conversations[session_id].append(message)
            # ãƒ¡ãƒ¢ãƒªå†…ãªã®ã§ã€ãƒ‡ã‚£ã‚¹ã‚¯ãƒ•ãƒ©ãƒƒã‚·ãƒ¥ã¯ä¸è¦ï¼ˆç©ºå®Ÿè£…ï¼‰

    def get_conversation(self, session_id: str) -> List[ConversationMessage]:
        """ä¼šè©±å±¥æ­´ã‚’å–å¾—"""
        with self.lock:
            return list(self.conversations.get(session_id, []))

    def clear_conversation(self, session_id: str):
        """ä¼šè©±å±¥æ­´ã‚’ã‚¯ãƒªã‚¢"""
        with self.lock:
            if session_id in self.conversations:
                del self.conversations[session_id]

    def start_new_session(self, session_id: str):
        """æ–°ã—ã„ã‚»ãƒƒã‚·ãƒ§ãƒ³ã‚’é–‹å§‹ï¼ˆæ—¢å­˜ã®ä¼šè©±ã‚’ã‚¯ãƒªã‚¢ï¼‰"""
        with self.lock:
            self.conversations[session_id] = []

    def _flush_to_disk(self):
        """ãƒ¡ãƒ¢ãƒªå†…ã‚¯ãƒ©ã‚¹ãªã®ã§ä½•ã‚‚ã—ãªã„ï¼ˆäº’æ›æ€§ã®ãŸã‚ï¼‰"""
        pass


class TaskAIService:
    """ã‚¿ã‚¹ã‚¯ã‚³ãƒ³ãƒ†ãƒ³ãƒ„AIæ‹¡å¼µã‚µãƒ¼ãƒ“ã‚¹"""

    def __init__(self, api_key: str, timeout_seconds: float = 30.0, model_name: str = "gemini-2.5-flash", history_storage_path: Optional[str] = None):
        self.client = genai.Client(api_key=api_key)
        # ãƒ¡ãƒ¢ãƒªå†…ã®ã¿ã§ä¼šè©±å±¥æ­´ã‚’ç®¡ç†ï¼ˆãƒ•ã‚©ãƒ¼ãƒ å…¥åŠ›æ™‚ã®ã¿ã®ä¸€æ™‚çš„ãªä½¿ç”¨ï¼‰
        self.history = InMemoryConversationHistory()
        self.timeout_seconds = timeout_seconds
        self.model_name = model_name
        self.max_retries = 3
        
        # ã‚·ã‚¹ãƒ†ãƒ æŒ‡ç¤ºï¼ˆç°¡æ½”ã‹ã¤æ§‹é€ åŒ–å¿œç­”ã‚’å¼·åˆ¶ï¼‰
        self.system_instruction = """ã‚ãªãŸã¯ã‚¿ã‚¹ã‚¯ç®¡ç†ã®è£œåŠ©AIã§ã™ã€‚æä¾›ã•ã‚ŒãŸæƒ…å ±ã‚’ã‚‚ã¨ã«ã€å®Ÿè¡Œå¯èƒ½ãªã‚¿ã‚¹ã‚¯ææ¡ˆã‚’è¡Œã„ã¾ã™ã€‚

å¿…ãšæ¬¡ã®ãƒ«ãƒ¼ãƒ«ã«å¾“ã£ã¦ãã ã•ã„ï¼š
- è¿”ç­”ã¯JSONã®ã¿ã€‚å‰å¾Œã«èª¬æ˜ã‚„ã‚³ãƒ¼ãƒ‰ãƒ–ãƒ­ãƒƒã‚¯ã€ã‚³ãƒ¡ãƒ³ãƒˆã¯ä»˜ä¸ã—ãªã„ã€‚
- ã‚¹ã‚­ãƒ¼ãƒã«æº–æ‹ ï¼šstatusã¯"insufficient_info"ã¾ãŸã¯"ready_to_format"ã€‚
- insufficientã®å ´åˆã€reasonã¨å…·ä½“çš„ãªquestionsé…åˆ—ï¼ˆç°¡æ½”ãªæ—¥æœ¬èªã®è³ªå•æ–‡ï¼‰ã‚’è¿”ã™ã€‚
- readyã®å ´åˆã€suggestion.descriptionã«ãƒãƒ¼ã‚¯ãƒ€ã‚¦ãƒ³å½¢å¼ã§ä»¥ä¸‹ã®é †åºã§è¨˜è¿°ã™ã‚‹ï¼ˆå¿…ãšå„ã‚»ã‚¯ã‚·ãƒ§ãƒ³é–“ã«æ”¹è¡Œ\\nã‚’å…¥ã‚Œã‚‹ï¼‰ï¼š
  ## ç›®çš„ãƒ»èƒŒæ™¯\\nï¼ˆç›®çš„ã‚„èƒŒæ™¯ã‚’è¨˜è¿°ï¼‰\\n\\n## ä½œæ¥­å†…å®¹\\n1. ï¼ˆå…·ä½“çš„ãªæ‰‹é †1ï¼‰\\n2. ï¼ˆå…·ä½“çš„ãªæ‰‹é †2ï¼‰\\n\\n## å®Œäº†æ¡ä»¶\\nï¼ˆå®Œäº†ã®åˆ¤æ–­åŸºæº–ï¼‰\\n\\n## æ³¨æ„ç‚¹\\nï¼ˆé‡è¦ãªæ³¨æ„äº‹é …ï¼‰
  å¯èƒ½ãªã‚‰title, category, urgency, due_date_isoã‚‚è£œå®Œã™ã‚‹ï¼ˆä¸æ˜ãªã‚‰çœç•¥å¯ï¼‰ã€‚

åˆ†é¡ã®æŒ‡é‡ï¼ˆå‚è€ƒï¼‰ï¼š
- ç¤¾å†…ã‚¿ã‚¹ã‚¯ / æŠ€è¡“èª¿æŸ» / é¡§å®¢å¯¾å¿œ / å–¶æ¥­é€£çµ¡ / è¦ä»¶å®šç¾© / è³‡æ–™ä½œæˆ / ãã®ä»–

ç°¡æ½”ã§ã€ã™ãå®Ÿè¡Œå¯èƒ½ãªå½¢ã«æ•´ãˆã¦ãã ã•ã„ã€‚"""

        # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã®ãƒ¢ãƒ‡ãƒ«åï¼ˆä¸Šä½ã‹ã‚‰æ³¨å…¥ã•ã‚Œã‚‹æƒ³å®šï¼‰
        if not hasattr(self, "model_name"):
            self.model_name = "gemini-2.5-flash"
    
    def _response_schema(self) -> types.Schema:
        """Geminiã®æ§‹é€ åŒ–å‡ºåŠ›ã‚¹ã‚­ãƒ¼ãƒã‚’å®šç¾©"""
        return types.Schema(
            type=types.Type.OBJECT,
            properties={
                "status": types.Schema(type=types.Type.STRING, enum=["insufficient_info", "ready_to_format"]),
                "reason": types.Schema(type=types.Type.STRING),
                "questions": types.Schema(
                    type=types.Type.ARRAY,
                    items=types.Schema(type=types.Type.STRING),
                ),
                "suggestion": types.Schema(
                    type=types.Type.OBJECT,
                    properties={
                        "title": types.Schema(type=types.Type.STRING),
                        "category": types.Schema(type=types.Type.STRING),
                        "urgency": types.Schema(type=types.Type.STRING),
                        "due_date_iso": types.Schema(type=types.Type.STRING),
                        "description": types.Schema(type=types.Type.STRING),
                    },
                ),
            },
        )

    def _build_contents(self, session_id: str, user_text: Optional[str] = None) -> List[types.Content]:
        """å±¥æ­´ + ç›´è¿‘ãƒ¦ãƒ¼ã‚¶ãƒ¼æŒ‡ç¤ºã‹ã‚‰Contentsã‚’ä½œã‚‹"""
        contents: List[types.Content] = []
        conversation = self.history.get_conversation(session_id)

        print(f"ğŸ” [_build_contents] ã‚»ãƒƒã‚·ãƒ§ãƒ³ {session_id}: å±¥æ­´æ•°={len(conversation)}")

        for i, msg in enumerate(conversation):
            role = "user" if msg.role == "user" else "model"
            print(f"  å±¥æ­´[{i}] {role}: {msg.content[:100]}...")
            contents.append(
                types.Content(role=role, parts=[types.Part.from_text(text=msg.content)])
            )

        if user_text:
            print(f"  æ–°è¦ãƒ¦ãƒ¼ã‚¶ãƒ¼å…¥åŠ›: {user_text[:100]}...")
            contents.append(types.Content(role="user", parts=[types.Part.from_text(text=user_text)]))

        print(f"ğŸ” [_build_contents] æœ€çµ‚çš„ãªcontentsæ•°: {len(contents)}")
        return contents

    def _call_ai_with_timeout(self, contents: Union[str, List[types.Content]], timeout: Optional[float] = None) -> str:
        """ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆ + ãƒªãƒˆãƒ©ã‚¤ä»˜ãã§AIã‚’å‘¼ã³å‡ºã™"""
        effective_timeout = timeout or self.timeout_seconds
        def call_ai():
            attempts = self.max_retries
            last_err: Optional[Exception] = None
            for i in range(attempts):
                try:
                    response = self.client.models.generate_content(
                        model=self.model_name,
                        contents=contents,
                        config=types.GenerateContentConfig(
                            thinking_config=types.ThinkingConfig(thinking_budget=0),
                            max_output_tokens=1000,
                            temperature=0.2,
                            system_instruction=self.system_instruction,
                            response_mime_type="application/json",
                            response_schema=self._response_schema(),
                        ),
                    )
                    return response.text
                except Exception as e:
                    msg = str(e).lower()
                    retryable = any(k in msg for k in [
                        "unavailable", "overloaded", "please try again", "deadline", "temporarily", "resource exhausted", "rate"
                    ])
                    last_err = e
                    if retryable and i < attempts - 1:
                        # æŒ‡æ•°ãƒãƒƒã‚¯ã‚ªãƒ• + ã‚¸ãƒƒã‚¿
                        sleep_s = (0.6 * (2 ** i)) + random.uniform(0, 0.3)
                        time.sleep(sleep_s)
                        continue
                    raise

        with concurrent.futures.ThreadPoolExecutor() as executor:
            future = executor.submit(call_ai)
            try:
                return future.result(timeout=effective_timeout)
            except concurrent.futures.TimeoutError:
                raise Exception("AI processing timeout - å‡¦ç†æ™‚é–“ãŒé•·ã™ãã¾ã™")
    
    async def _call_ai_with_timeout_async(self, contents: Union[str, List[types.Content]], timeout: Optional[float] = None) -> str:
        """éåŒæœŸã§ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆ + ãƒªãƒˆãƒ©ã‚¤ä»˜ãã§AIã‚’å‘¼ã³å‡ºã™"""
        import asyncio
        effective_timeout = timeout or self.timeout_seconds
        
        def call_ai():
            attempts = self.max_retries
            last_err: Optional[Exception] = None
            for i in range(attempts):
                try:
                    response = self.client.models.generate_content(
                        model=self.model_name,
                        contents=contents,
                        config=types.GenerateContentConfig(
                            thinking_config=types.ThinkingConfig(thinking_budget=0),
                            max_output_tokens=1000,
                            temperature=0.2,
                            system_instruction=self.system_instruction,
                            response_mime_type="application/json",
                            response_schema=self._response_schema(),
                        ),
                    )
                    return response.text
                except Exception as e:
                    last_err = e
                    print(f"âŒ AI call attempt {i+1}/{attempts} failed: {e}")
                    if i < attempts - 1:
                        time.sleep(2 ** i)  # æŒ‡æ•°ãƒãƒƒã‚¯ã‚ªãƒ•
            raise last_err or Exception("All attempts failed")
        
        # åˆ¥ã‚¹ãƒ¬ãƒƒãƒ‰ã§å®Ÿè¡Œã—ã¦éãƒ–ãƒ­ãƒƒã‚­ãƒ³ã‚°åŒ–
        try:
            loop = asyncio.get_event_loop()
            return await loop.run_in_executor(None, call_ai)
        except asyncio.TimeoutError:
            raise Exception("AI processing timeout - å‡¦ç†æ™‚é–“ãŒé•·ã™ãã¾ã™")
    
    async def analyze_task_info(self, session_id: str, task_info: TaskInfo) -> AIAnalysisResult:
        """ã‚¿ã‚¹ã‚¯æƒ…å ±ã‚’åˆ†æ"""
        try:
            print(f"ğŸ¤– AIåˆ†æé–‹å§‹: session_id={session_id}")
            # ç¾åœ¨ã®ã‚¿ã‚¹ã‚¯æƒ…å ±ã‚’ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã«æ•´ç†
            prompt = self._build_analysis_prompt(task_info)
            print(f"ğŸ” ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆä½œæˆå®Œäº†: {len(prompt)}æ–‡å­—")
            # å±¥æ­´ã«ãƒ¦ãƒ¼ã‚¶ãƒ¼ç™ºè©±ã‚’è¿½åŠ ã—ã€å±¥æ­´è¾¼ã¿ã®contentsã‚’æ§‹ç¯‰
            self.history.add_message(session_id, "user", prompt)
            contents = self._build_contents(session_id)
            print(f"ğŸ” ã‚³ãƒ³ãƒ†ãƒ³ãƒ„æ§‹ç¯‰å®Œäº†: {len(str(contents))}æ–‡å­—")
            # ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆï¼ˆè¨­å®šå€¤ï¼‰ä»˜ãã§Gemini APIã«é€ä¿¡ï¼ˆæ§‹é€ åŒ–JSONã‚’æœŸå¾…ï¼‰
            print("ğŸ” Gemini APIå‘¼ã³å‡ºã—é–‹å§‹...")
            response_text = await self._call_ai_with_timeout_async(contents)
            print(f"âœ… Gemini APIå‘¼ã³å‡ºã—å®Œäº†: {len(response_text)}æ–‡å­—")
            
            # ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã‚’ä¼šè©±å±¥æ­´ã«è¿½åŠ 
            self.history.add_message(session_id, "model", response_text)
            
            # ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã‚’è§£æ
            print("ğŸ” ãƒ¬ã‚¹ãƒãƒ³ã‚¹è§£æä¸­...")
            result = self._parse_ai_response(response_text)
            print(f"âœ… AIåˆ†æå®Œäº†: status={result.status}")
            return result
            
        except Exception as e:
            print(f"âŒ AI analysis error: {e}")
            return AIAnalysisResult(
                status="error",
                message=f"AIåˆ†æã§ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {str(e)}"
            )
    
    async def refine_content(self, session_id: str, feedback: str) -> AIAnalysisResult:
        """ãƒ¦ãƒ¼ã‚¶ãƒ¼ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯ã‚’åŸºã«ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’æ”¹è‰¯"""
        try:
            print(f"ğŸ”„ AIæ”¹è‰¯é–‹å§‹: session_id={session_id}")
            user_turn = f"ä»¥ä¸‹ã®ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯ã‚’åæ˜ ã—ã¦æ”¹å–„ã—ã¦ãã ã•ã„ã€‚å¿…è¦ãªã‚‰ä¸è¶³ç‚¹ã‚‚è³ªå•ã—ã¦ãã ã•ã„ã€‚\n{feedback}"
            # å±¥æ­´ã«ãƒ¦ãƒ¼ã‚¶ãƒ¼ç™ºè©±ã‚’è¿½åŠ ã—ã€å±¥æ­´è¾¼ã¿ã®contentsã‚’æ§‹ç¯‰
            self.history.add_message(session_id, "user", user_turn)
            contents = self._build_contents(session_id)
            # ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆï¼ˆè¨­å®šå€¤ï¼‰ä»˜ãã§Gemini APIã«é€ä¿¡ï¼ˆæ§‹é€ åŒ–JSONã‚’æœŸå¾…ï¼‰
            print("ğŸ” Gemini APIå‘¼ã³å‡ºã—é–‹å§‹ï¼ˆæ”¹è‰¯ï¼‰...")
            response_text = await self._call_ai_with_timeout_async(contents)
            print(f"âœ… Gemini APIå‘¼ã³å‡ºã—å®Œäº†ï¼ˆæ”¹è‰¯ï¼‰: {len(response_text)}æ–‡å­—")
            
            # ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã‚’ä¼šè©±å±¥æ­´ã«è¿½åŠ 
            self.history.add_message(session_id, "model", response_text)
            
            result = self._parse_ai_response(response_text)
            print(f"âœ… AIæ”¹è‰¯å®Œäº†: status={result.status}")
            return result
            
        except Exception as e:
            print(f"âŒ AI refinement error: {e}")
            return AIAnalysisResult(
                status="error", 
                message=f"AIæ”¹è‰¯ã§ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {str(e)}"
            )
    
    def clear_session(self, session_id: str):
        """ã‚»ãƒƒã‚·ãƒ§ãƒ³ã‚’ã‚¯ãƒªã‚¢"""
        self.history.clear_conversation(session_id)
    
    def _build_analysis_prompt(self, task_info: TaskInfo) -> str:
        """åˆ†æç”¨ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’æ§‹ç¯‰"""
        prompt_parts = [
            "ä»¥ä¸‹ã®ã‚¿ã‚¹ã‚¯æƒ…å ±ã‚’åˆ†æã—ã¦ãã ã•ã„ï¼š",
            "",
            f"ã‚¿ã‚¤ãƒˆãƒ«: {task_info.title}"
        ]
        
        if task_info.task_type:
            prompt_parts.append(f"ã‚¿ã‚¹ã‚¯ç¨®é¡: {task_info.task_type}")
        if task_info.urgency:
            prompt_parts.append(f"ç·Šæ€¥åº¦: {task_info.urgency}")
        if task_info.due_date:
            prompt_parts.append(f"ç´æœŸ: {task_info.due_date}")
        if task_info.current_description:
            prompt_parts.append(f"ç¾åœ¨ã®å†…å®¹: {task_info.current_description}")
        
        return "\n".join(prompt_parts)
    
    def _parse_ai_response(self, response_text: str) -> AIAnalysisResult:
        """AIãƒ¬ã‚¹ãƒãƒ³ã‚¹ã‚’è§£æï¼ˆJSONå„ªå…ˆã€å¤±æ•—æ™‚ã¯ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼‰"""
        # 1) ã¾ãšJSONã¨ã—ã¦è§£é‡ˆ
        try:
            data = json.loads(response_text)
            status = data.get("status")

            if status == "insufficient_info":
                reason = data.get("reason") or "è¿½åŠ æƒ…å ±ãŒå¿…è¦ã§ã™ã€‚"
                questions = data.get("questions") or []
                # æ–‡å­—åˆ—ã§æ¥ã‚‹ã“ã¨ã‚‚è€ƒæ…®
                if isinstance(questions, str):
                    questions = [questions]
                return AIAnalysisResult(
                    status="insufficient_info",
                    message=reason,
                    suggestions=questions,
                )

            if status in ("ready_to_format", "ready"):
                suggestion = data.get("suggestion") or {}
                desc = suggestion.get("description")
                if not desc:
                    # æœ€ä½é™ã®æ•´å½¢ã‚’è¡Œã†
                    title = suggestion.get("title") or "ã‚¿ã‚¹ã‚¯"
                    category = suggestion.get("category")
                    urgency = suggestion.get("urgency")
                    due = suggestion.get("due_date_iso")
                    meta = []
                    if category:
                        meta.append(f"ã‚«ãƒ†ã‚´ãƒª: {category}")
                    if urgency:
                        meta.append(f"ç·Šæ€¥åº¦: {urgency}")
                    if due:
                        meta.append(f"ç´æœŸ: {due}")
                    meta_text = ("\n" + " / ".join(meta)) if meta else ""
                    desc = f"ã€{title}ã€‘{meta_text}\n\n## ç›®çš„ãƒ»èƒŒæ™¯\nä¸æ˜ç¢ºãªç‚¹ã¯ã‚ã‚Šã¾ã›ã‚“ã€‚\n\n## ä½œæ¥­å†…å®¹\n1. å¿…è¦ãªæ‰‹é †ã‚’å®Ÿæ–½ã—ã¦ãã ã•ã„ã€‚\n\n## å®Œäº†æ¡ä»¶\nåˆæ„æ¸ˆã¿ã®å—ã‘å…¥ã‚ŒåŸºæº–ã‚’æº€ãŸã™ã“ã¨ã€‚\n\n## æ³¨æ„ç‚¹\né–¢ä¿‚è€…ã¨ã®èªè­˜åˆã‚ã›ã‚’è¡Œã£ã¦ãã ã•ã„ã€‚"

                return AIAnalysisResult(
                    status="ready_to_format",
                    message="ç”Ÿæˆã«æˆåŠŸã—ã¾ã—ãŸ",
                    formatted_content=desc.strip(),
                )
        except Exception:
            pass

        # 2) ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼šã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãƒ™ãƒ¼ã‚¹
        try:
            lines = response_text.split("\n")
            if any(keyword in response_text.lower() for keyword in ["ä¸è¶³", "è¶³ã‚Šãªã„", "å¿…è¦ã§ã™", "æ•™ãˆã¦", "ã©ã®"]):
                suggestions = []
                for line in lines:
                    if "?" in line or "ï¼Ÿ" in line or line.strip().startswith("-"):
                        suggestions.append(line.strip())
                if not suggestions:
                    suggestions = ["è¿½åŠ ã®æƒ…å ±ã‚’æ•™ãˆã¦ãã ã•ã„ã€‚"]
                return AIAnalysisResult(
                    status="insufficient_info",
                    message=response_text,
                    suggestions=suggestions,
                )
            # ãã‚Œä»¥å¤–ã¯å®Œæˆã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã¨ã—ã¦æ‰±ã†
            return AIAnalysisResult(
                status="ready_to_format",
                message="ç”Ÿæˆã«æˆåŠŸã—ã¾ã—ãŸ",
                formatted_content=response_text.strip(),
            )
        except Exception as e:
            print(f"âŒ Response parsing error: {e}")
            return AIAnalysisResult(status="error", message=f"ãƒ¬ã‚¹ãƒãƒ³ã‚¹è§£æã‚¨ãƒ©ãƒ¼: {str(e)}")
